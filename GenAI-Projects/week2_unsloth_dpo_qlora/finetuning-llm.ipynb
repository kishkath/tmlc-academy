{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12884721,"sourceType":"datasetVersion","datasetId":8151782},{"sourceId":13250636,"sourceType":"datasetVersion","datasetId":8396227}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"print(\"Starting Finetuning...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T03:45:20.756787Z","iopub.execute_input":"2025-10-05T03:45:20.756978Z","iopub.status.idle":"2025-10-05T03:45:20.764118Z","shell.execute_reply.started":"2025-10-05T03:45:20.756953Z","shell.execute_reply":"2025-10-05T03:45:20.763449Z"}},"outputs":[{"name":"stdout","text":"Starting Finetuning...\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import kaggle_secrets\nimport os\nimport wandb\n\n# Fetch API key from Kaggle Secrets\nWANDB_API_KEY = kaggle_secrets.UserSecretsClient().get_secret(\"WANDB_API_KEY\")\n\n# Set as environment variable\nos.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n\n# Login to W&B\nwandb.login(key=WANDB_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T03:45:20.765695Z","iopub.execute_input":"2025-10-05T03:45:20.765880Z","iopub.status.idle":"2025-10-05T03:45:30.116038Z","shell.execute_reply.started":"2025-10-05T03:45:20.765865Z","shell.execute_reply":"2025-10-05T03:45:30.115428Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkiranchw000\u001b[0m (\u001b[33mimnskc\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"## install requirements\n# !bash /kaggle/working/TMLC/week2_unsloth_dpo_qlora/requirements.sh","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T03:46:24.222891Z","iopub.execute_input":"2025-10-05T03:46:24.223495Z","iopub.status.idle":"2025-10-05T03:46:24.226583Z","shell.execute_reply.started":"2025-10-05T03:46:24.223470Z","shell.execute_reply":"2025-10-05T03:46:24.225906Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"%%capture\n!pip install pip3-autoremove\n!pip install datasets trl wandb\n!pip-autoremove torch torchvision torchaudio -y\n!pip install \"torch==2.4.0\" \"xformers==0.0.27.post2\" triton torchvision torchaudio\n!pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T03:46:33.031904Z","iopub.execute_input":"2025-10-05T03:46:33.032706Z","iopub.status.idle":"2025-10-05T03:50:57.332552Z","shell.execute_reply.started":"2025-10-05T03:46:33.032677Z","shell.execute_reply":"2025-10-05T03:50:57.331686Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import shutil\nimport os\n\n# # Example: remove a directory and all its contents\ndir_path = \"/kaggle/working\"\n\n# removing current directories to run this version\n!rm -rf /kaggle/working/*\n\nprint(os.listdir(dir_path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T03:52:48.638079Z","iopub.execute_input":"2025-10-05T03:52:48.638864Z","iopub.status.idle":"2025-10-05T03:52:48.775390Z","shell.execute_reply.started":"2025-10-05T03:52:48.638830Z","shell.execute_reply":"2025-10-05T03:52:48.774452Z"}},"outputs":[{"name":"stdout","text":"['.virtual_documents']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!git clone https://github.com/kishkath/TMLC.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T03:52:55.136238Z","iopub.execute_input":"2025-10-05T03:52:55.136564Z","iopub.status.idle":"2025-10-05T03:52:55.784600Z","shell.execute_reply.started":"2025-10-05T03:52:55.136533Z","shell.execute_reply":"2025-10-05T03:52:55.783606Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'TMLC'...\nremote: Enumerating objects: 637, done.\u001b[K\nremote: Counting objects: 100% (171/171), done.\u001b[K\nremote: Compressing objects: 100% (171/171), done.\u001b[K\nremote: Total 637 (delta 111), reused 0 (delta 0), pack-reused 466 (from 2)\u001b[K\nReceiving objects: 100% (637/637), 391.62 KiB | 5.09 MiB/s, done.\nResolving deltas: 100% (372/372), done.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# !pip install unsloth_zoo","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T03:54:50.364633Z","iopub.execute_input":"2025-10-05T03:54:50.364964Z","iopub.status.idle":"2025-10-05T03:54:50.369087Z","shell.execute_reply.started":"2025-10-05T03:54:50.364935Z","shell.execute_reply":"2025-10-05T03:54:50.368463Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"!python /kaggle/working/TMLC/week2_unsloth_dpo_qlora/main.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T03:54:52.074056Z","iopub.execute_input":"2025-10-05T03:54:52.074460Z","iopub.status.idle":"2025-10-05T05:43:18.896173Z","shell.execute_reply.started":"2025-10-05T03:54:52.074424Z","shell.execute_reply":"2025-10-05T05:43:18.895374Z"}},"outputs":[{"name":"stdout","text":"ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n2025-10-05 03:55:05.382617: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759636505.783971     191 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759636505.889085     191 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nü¶• Unsloth Zoo will now patch everything to make training faster!\n2025-10-05 03:55:38,138 | INFO | Preparing dataset...\nGenerating train split: 6000 examples [00:00, 33793.74 examples/s]\nGenerating val split: 1000 examples [00:00, 66216.79 examples/s]\nMap: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:00<00:00, 18526.80 examples/s]\nMap: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 18525.34 examples/s]\nFilter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:00<00:00, 111351.23 examples/s]\nFilter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 134119.02 examples/s]\n2025-10-05 03:55:39,094 | INFO | ‚úÖ Train size: 4200\n2025-10-05 03:55:39,094 | INFO | ‚úÖ Test size: 500\n2025-10-05 03:55:39,096 | INFO | ‚úÖ Example record: {'question': 'Should I focus on protein intake at maintenance calories if my goal is performance and health?', 'chosen': 'Prioritize total daily protein, calorie balance, and plenty of fruits and vegetables. Tailor meal timing around training for energy and recovery, but consistency across the week matters most. Monitor weekly trends (body weight, strength, energy) and adjust one variable at a time. If you have medical conditions or take medications, consult a qualified professional before making changes.', 'rejected': 'Prioritize total daily protein, calorie balance, and plenty of fruits and vegetables without considering health implications.', 'prompt': 'Should I focus on protein intake at maintenance calories if my goal is performance and health?'}\n2025-10-05 03:55:39,096 | INFO | Loading models...\n2025-10-05 03:55:39,096 | INFO | Loading model 'unsloth/Qwen3-0.6B-unsloth-bnb-4bit' with max_seq_length=2048\n==((====))==  Unsloth 2025.9.11: Fast Qwen3 patching. Transformers: 4.56.2.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.4.0+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.0.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.27.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nmodel.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 576M/576M [00:02<00:00, 245MB/s]\ngeneration_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 237/237 [00:00<00:00, 2.47MB/s]\ntokenizer_config.json: 10.5kB [00:00, 45.8MB/s]\nvocab.json: 2.78MB [00:00, 45.9MB/s]\nmerges.txt: 1.67MB [00:00, 93.1MB/s]\nadded_tokens.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 707/707 [00:00<00:00, 9.10MB/s]\nspecial_tokens_map.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 614/614 [00:00<00:00, 8.58MB/s]\ntokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11.4M/11.4M [00:00<00:00, 31.0MB/s]\nchat_template.jinja: 4.91kB [00:00, 21.2MB/s]\n2025-10-05 03:55:48,461 | INFO | ‚úÖ Base model loaded\nUnsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\nUnsloth will patch all other layers, except LoRA matrices, causing a performance hit.\nUnsloth 2025.9.11 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n2025-10-05 03:55:51,732 | INFO | ‚úÖ LoRA adapters attached\n2025-10-05 03:55:51,732 | INFO | Loading reference model 'unsloth/Qwen3-0.6B-unsloth-bnb-4bit'\n==((====))==  Unsloth 2025.9.11: Fast Qwen3 patching. Transformers: 4.56.2.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.4.0+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.0.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.27.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n2025-10-05 03:55:56,572 | INFO | ‚úÖ Reference model loaded\n2025-10-05 03:55:56,572 | INFO | ‚úÖ Models loaded successfully\n2025-10-05 03:55:56,572 | INFO | Setting random seed...\n2025-10-05 03:55:56,573 | INFO | Setting seed: 42\n2025-10-05 03:55:56,573 | INFO | Creating trainer...\n2025-10-05 03:55:56,573 | INFO | Creating DPO Trainer...\nExtracting prompt in train dataset (num_proc=8): 100%|‚ñà| 4200/4200 [00:00<00:00,\nApplying chat template to train dataset (num_proc=8): 100%|‚ñà| 4200/4200 [00:02<0\nTokenizing train dataset (num_proc=8): 100%|‚ñà| 4200/4200 [00:04<00:00, 1011.70 e\nExtracting prompt in eval dataset (num_proc=8): 100%|‚ñà| 500/500 [00:00<00:00, 18\nApplying chat template to eval dataset (num_proc=8): 100%|‚ñà| 500/500 [00:02<00:0\nTokenizing eval dataset (num_proc=8): 100%|‚ñà| 500/500 [00:02<00:00, 172.33 examp\n2025-10-05 03:56:12,768 | INFO | Initializing WandB project: fitness-bot, run name: qwen3_0.6b_finetuning\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkiranchw000\u001b[0m (\u001b[33mimnskc\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.20.1\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20251005_035612-2wycpige\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mqwen3_0.6b_finetuning\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/imnskc/fitness-bot\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/imnskc/fitness-bot/runs/2wycpige\u001b[0m\n2025-10-05 03:56:14,962 | INFO | ‚úÖ DPO Trainer created successfully.\n2025-10-05 03:56:14,962 | INFO | ‚úÖ Trainer created successfully\n2025-10-05 03:56:14,962 | INFO | üöÄ Starting training...\nThe tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 4,200 | Num Epochs = 5 | Total steps = 2,625\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n \"-____-\"     Trainable parameters = 10,092,544 of 606,142,464 (1.67% trained)\n{'loss': 0.5866, 'grad_norm': 15.376080513000488, 'learning_rate': 9.315589353612169e-07, 'rewards/chosen': 0.24619625508785248, 'rewards/rejected': -0.0012744207633659244, 'rewards/accuracies': 0.8725000023841858, 'rewards/margins': 0.2474706619977951, 'logps/chosen': -323.2349548339844, 'logps/rejected': -80.80519104003906, 'logits/chosen': -1.466444730758667, 'logits/rejected': -0.7429612874984741, 'epoch': 0.1}\n{'loss': 0.1297, 'grad_norm': 0.7401503324508667, 'learning_rate': 1.88212927756654e-06, 'rewards/chosen': 2.3461294174194336, 'rewards/rejected': -0.023017050698399544, 'rewards/accuracies': 1.0, 'rewards/margins': 2.3691463470458984, 'logps/chosen': -297.9217529296875, 'logps/rejected': -84.1123275756836, 'logits/chosen': -1.6623202562332153, 'logits/rejected': -0.8510066270828247, 'epoch': 0.19}\n{'loss': 0.0066, 'grad_norm': 0.13281580805778503, 'learning_rate': 2.832699619771863e-06, 'rewards/chosen': 5.3165411949157715, 'rewards/rejected': -0.2151395082473755, 'rewards/accuracies': 1.0, 'rewards/margins': 5.531680107116699, 'logps/chosen': -270.83892822265625, 'logps/rejected': -84.23873138427734, 'logits/chosen': -1.9263726472854614, 'logits/rejected': -0.9388431310653687, 'epoch': 0.29}\n{'loss': 0.0016, 'grad_norm': 0.05191473662853241, 'learning_rate': 3.783269961977187e-06, 'rewards/chosen': 6.540058612823486, 'rewards/rejected': -0.3326396644115448, 'rewards/accuracies': 1.0, 'rewards/margins': 6.8726983070373535, 'logps/chosen': -253.31251525878906, 'logps/rejected': -85.89761352539062, 'logits/chosen': -2.1161742210388184, 'logits/rejected': -1.0675050020217896, 'epoch': 0.38}\n{'loss': 0.0007, 'grad_norm': 0.028661789372563362, 'learning_rate': 4.73384030418251e-06, 'rewards/chosen': 7.2767109870910645, 'rewards/rejected': -0.4851647913455963, 'rewards/accuracies': 1.0, 'rewards/margins': 7.761875629425049, 'logps/chosen': -252.3098602294922, 'logps/rejected': -86.554443359375, 'logits/chosen': -2.1703550815582275, 'logits/rejected': -1.0690319538116455, 'epoch': 0.48}\n{'loss': 0.0005, 'grad_norm': 0.03368206322193146, 'learning_rate': 4.923793395427604e-06, 'rewards/chosen': 7.724875450134277, 'rewards/rejected': -0.4961874485015869, 'rewards/accuracies': 1.0, 'rewards/margins': 8.221063613891602, 'logps/chosen': -239.1215057373047, 'logps/rejected': -89.81303405761719, 'logits/chosen': -2.255237579345703, 'logits/rejected': -1.1531705856323242, 'epoch': 0.57}\n{'loss': 0.0003, 'grad_norm': 0.008121545426547527, 'learning_rate': 4.817950889077054e-06, 'rewards/chosen': 8.091863632202148, 'rewards/rejected': -0.6572486162185669, 'rewards/accuracies': 1.0, 'rewards/margins': 8.749112129211426, 'logps/chosen': -237.1905059814453, 'logps/rejected': -86.71226501464844, 'logits/chosen': -2.296504497528076, 'logits/rejected': -1.1444652080535889, 'epoch': 0.67}\n{'loss': 0.0002, 'grad_norm': 0.006722116842865944, 'learning_rate': 4.712108382726504e-06, 'rewards/chosen': 8.452095031738281, 'rewards/rejected': -0.6569333076477051, 'rewards/accuracies': 1.0, 'rewards/margins': 9.109027862548828, 'logps/chosen': -241.42408752441406, 'logps/rejected': -87.14020538330078, 'logits/chosen': -2.3429503440856934, 'logits/rejected': -1.1798434257507324, 'epoch': 0.76}\n{'loss': 0.0001, 'grad_norm': 0.014477936550974846, 'learning_rate': 4.606265876375953e-06, 'rewards/chosen': 8.654716491699219, 'rewards/rejected': -0.705708920955658, 'rewards/accuracies': 1.0, 'rewards/margins': 9.36042594909668, 'logps/chosen': -237.16302490234375, 'logps/rejected': -88.85408020019531, 'logits/chosen': -2.342305898666382, 'logits/rejected': -1.1544291973114014, 'epoch': 0.86}\n{'loss': 0.0001, 'grad_norm': 0.006193196401000023, 'learning_rate': 4.5004233700254025e-06, 'rewards/chosen': 8.827596664428711, 'rewards/rejected': -0.6934800148010254, 'rewards/accuracies': 1.0, 'rewards/margins': 9.521076202392578, 'logps/chosen': -237.2291259765625, 'logps/rejected': -89.82295227050781, 'logits/chosen': -2.380441188812256, 'logits/rejected': -1.2230170965194702, 'epoch': 0.95}\n{'loss': 0.0001, 'grad_norm': 0.0042740823701024055, 'learning_rate': 4.394580863674852e-06, 'rewards/chosen': 8.967842102050781, 'rewards/rejected': -0.7253986597061157, 'rewards/accuracies': 1.0, 'rewards/margins': 9.69324016571045, 'logps/chosen': -230.2250518798828, 'logps/rejected': -90.62105560302734, 'logits/chosen': -2.437803268432617, 'logits/rejected': -1.2335760593414307, 'epoch': 1.05}\n{'loss': 0.0001, 'grad_norm': 0.006390917114913464, 'learning_rate': 4.288738357324302e-06, 'rewards/chosen': 9.107295036315918, 'rewards/rejected': -0.7335735559463501, 'rewards/accuracies': 1.0, 'rewards/margins': 9.840868949890137, 'logps/chosen': -228.48948669433594, 'logps/rejected': -91.6726303100586, 'logits/chosen': -2.4404616355895996, 'logits/rejected': -1.2723979949951172, 'epoch': 1.14}\n{'loss': 0.0001, 'grad_norm': 0.004483222495764494, 'learning_rate': 4.182895850973752e-06, 'rewards/chosen': 9.310124397277832, 'rewards/rejected': -0.808072030544281, 'rewards/accuracies': 1.0, 'rewards/margins': 10.118197441101074, 'logps/chosen': -233.44908142089844, 'logps/rejected': -90.15542602539062, 'logits/chosen': -2.446772813796997, 'logits/rejected': -1.2488229274749756, 'epoch': 1.24}\n{'loss': 0.0001, 'grad_norm': 0.002721252618357539, 'learning_rate': 4.077053344623201e-06, 'rewards/chosen': 9.39643669128418, 'rewards/rejected': -0.8300774097442627, 'rewards/accuracies': 1.0, 'rewards/margins': 10.226513862609863, 'logps/chosen': -227.07875061035156, 'logps/rejected': -91.20585632324219, 'logits/chosen': -2.4928886890411377, 'logits/rejected': -1.2595760822296143, 'epoch': 1.33}\n{'loss': 0.0001, 'grad_norm': 0.005792133044451475, 'learning_rate': 3.9712108382726505e-06, 'rewards/chosen': 9.474969863891602, 'rewards/rejected': -0.8512595891952515, 'rewards/accuracies': 1.0, 'rewards/margins': 10.326229095458984, 'logps/chosen': -227.2316436767578, 'logps/rejected': -91.4040756225586, 'logits/chosen': -2.511570930480957, 'logits/rejected': -1.2861839532852173, 'epoch': 1.43}\n{'loss': 0.0001, 'grad_norm': 0.001917261746712029, 'learning_rate': 3.8653683319221e-06, 'rewards/chosen': 9.584980010986328, 'rewards/rejected': -0.9169562458992004, 'rewards/accuracies': 1.0, 'rewards/margins': 10.501936912536621, 'logps/chosen': -230.34738159179688, 'logps/rejected': -90.2007064819336, 'logits/chosen': -2.5148000717163086, 'logits/rejected': -1.270795464515686, 'epoch': 1.52}\n{'loss': 0.0, 'grad_norm': 0.0013824906200170517, 'learning_rate': 3.75952582557155e-06, 'rewards/chosen': 9.757811546325684, 'rewards/rejected': -0.9144942760467529, 'rewards/accuracies': 1.0, 'rewards/margins': 10.672306060791016, 'logps/chosen': -223.2587432861328, 'logps/rejected': -90.32098388671875, 'logits/chosen': -2.537062406539917, 'logits/rejected': -1.2258250713348389, 'epoch': 1.62}\n{'loss': 0.0, 'grad_norm': 0.0011834749020636082, 'learning_rate': 3.653683319221e-06, 'rewards/chosen': 9.789953231811523, 'rewards/rejected': -0.9385484457015991, 'rewards/accuracies': 1.0, 'rewards/margins': 10.72850227355957, 'logps/chosen': -222.2160186767578, 'logps/rejected': -91.70349884033203, 'logits/chosen': -2.609588861465454, 'logits/rejected': -1.3117303848266602, 'epoch': 1.71}\n{'loss': 0.0, 'grad_norm': 0.0017046573339030147, 'learning_rate': 3.5478408128704487e-06, 'rewards/chosen': 9.877520561218262, 'rewards/rejected': -0.9671483039855957, 'rewards/accuracies': 1.0, 'rewards/margins': 10.844669342041016, 'logps/chosen': -226.27154541015625, 'logps/rejected': -90.76284790039062, 'logits/chosen': -2.569425582885742, 'logits/rejected': -1.303816795349121, 'epoch': 1.81}\n{'loss': 0.0, 'grad_norm': 0.003684940515086055, 'learning_rate': 3.4419983065198985e-06, 'rewards/chosen': 9.87795639038086, 'rewards/rejected': -0.9898523092269897, 'rewards/accuracies': 1.0, 'rewards/margins': 10.867807388305664, 'logps/chosen': -225.8120880126953, 'logps/rejected': -92.08866119384766, 'logits/chosen': -2.5630462169647217, 'logits/rejected': -1.3118374347686768, 'epoch': 1.9}\n{'loss': 0.0, 'grad_norm': 0.0022048079408705235, 'learning_rate': 3.3361558001693482e-06, 'rewards/chosen': 9.924463272094727, 'rewards/rejected': -0.9343716502189636, 'rewards/accuracies': 1.0, 'rewards/margins': 10.858834266662598, 'logps/chosen': -220.89175415039062, 'logps/rejected': -91.60028076171875, 'logits/chosen': -2.6004679203033447, 'logits/rejected': -1.3442130088806152, 'epoch': 2.0}\n{'loss': 0.0, 'grad_norm': 0.0014630064833909273, 'learning_rate': 3.230313293818798e-06, 'rewards/chosen': 10.030500411987305, 'rewards/rejected': -1.0222584009170532, 'rewards/accuracies': 1.0, 'rewards/margins': 11.052759170532227, 'logps/chosen': -222.9873046875, 'logps/rejected': -94.00335693359375, 'logits/chosen': -2.6127676963806152, 'logits/rejected': -1.3105576038360596, 'epoch': 2.1}\n{'loss': 0.0, 'grad_norm': 0.003373190760612488, 'learning_rate': 3.1244707874682474e-06, 'rewards/chosen': 10.127381324768066, 'rewards/rejected': -1.007305383682251, 'rewards/accuracies': 1.0, 'rewards/margins': 11.134687423706055, 'logps/chosen': -224.14816284179688, 'logps/rejected': -90.81404113769531, 'logits/chosen': -2.610355854034424, 'logits/rejected': -1.3364094495773315, 'epoch': 2.19}\n{'loss': 0.0, 'grad_norm': 0.0026811915449798107, 'learning_rate': 3.018628281117697e-06, 'rewards/chosen': 10.225549697875977, 'rewards/rejected': -0.985149621963501, 'rewards/accuracies': 1.0, 'rewards/margins': 11.210700988769531, 'logps/chosen': -216.90625, 'logps/rejected': -92.90325164794922, 'logits/chosen': -2.6291913986206055, 'logits/rejected': -1.3109489679336548, 'epoch': 2.29}\n{'loss': 0.0, 'grad_norm': 0.0023092878982424736, 'learning_rate': 2.912785774767147e-06, 'rewards/chosen': 10.223895072937012, 'rewards/rejected': -1.0081958770751953, 'rewards/accuracies': 1.0, 'rewards/margins': 11.232091903686523, 'logps/chosen': -217.09432983398438, 'logps/rejected': -92.83078002929688, 'logits/chosen': -2.6625442504882812, 'logits/rejected': -1.341005802154541, 'epoch': 2.38}\n{'loss': 0.0, 'grad_norm': 0.0022513798903673887, 'learning_rate': 2.8069432684165966e-06, 'rewards/chosen': 10.273573875427246, 'rewards/rejected': -1.0542042255401611, 'rewards/accuracies': 1.0, 'rewards/margins': 11.327776908874512, 'logps/chosen': -223.8124237060547, 'logps/rejected': -92.22984313964844, 'logits/chosen': -2.6407885551452637, 'logits/rejected': -1.2997492551803589, 'epoch': 2.48}\n{'loss': 0.0, 'grad_norm': 0.0006012236699461937, 'learning_rate': 2.7011007620660464e-06, 'rewards/chosen': 10.26280403137207, 'rewards/rejected': -1.0316722393035889, 'rewards/accuracies': 1.0, 'rewards/margins': 11.294476509094238, 'logps/chosen': -218.62713623046875, 'logps/rejected': -91.45343780517578, 'logits/chosen': -2.6450326442718506, 'logits/rejected': -1.3661606311798096, 'epoch': 2.57}\n{'loss': 0.0, 'grad_norm': 0.002402176149189472, 'learning_rate': 2.5952582557154953e-06, 'rewards/chosen': 10.270232200622559, 'rewards/rejected': -1.0238900184631348, 'rewards/accuracies': 1.0, 'rewards/margins': 11.294120788574219, 'logps/chosen': -216.5167236328125, 'logps/rejected': -93.14977264404297, 'logits/chosen': -2.6520419120788574, 'logits/rejected': -1.3953251838684082, 'epoch': 2.67}\n{'loss': 0.0, 'grad_norm': 0.002978964475914836, 'learning_rate': 2.489415749364945e-06, 'rewards/chosen': 10.351129531860352, 'rewards/rejected': -1.079140067100525, 'rewards/accuracies': 1.0, 'rewards/margins': 11.430268287658691, 'logps/chosen': -219.14361572265625, 'logps/rejected': -92.95306396484375, 'logits/chosen': -2.6765048503875732, 'logits/rejected': -1.4139503240585327, 'epoch': 2.76}\n{'loss': 0.0, 'grad_norm': 0.0008072851924225688, 'learning_rate': 2.383573243014395e-06, 'rewards/chosen': 10.495126724243164, 'rewards/rejected': -1.1405044794082642, 'rewards/accuracies': 1.0, 'rewards/margins': 11.635629653930664, 'logps/chosen': -218.33587646484375, 'logps/rejected': -92.22027587890625, 'logits/chosen': -2.6686089038848877, 'logits/rejected': -1.3182480335235596, 'epoch': 2.86}\n{'loss': 0.0, 'grad_norm': 0.0012891889782622457, 'learning_rate': 2.277730736663844e-06, 'rewards/chosen': 10.47859001159668, 'rewards/rejected': -1.0307013988494873, 'rewards/accuracies': 1.0, 'rewards/margins': 11.509291648864746, 'logps/chosen': -216.82217407226562, 'logps/rejected': -94.17393493652344, 'logits/chosen': -2.6935486793518066, 'logits/rejected': -1.4036478996276855, 'epoch': 2.95}\n{'loss': 0.0, 'grad_norm': 0.0010933155426755548, 'learning_rate': 2.171888230313294e-06, 'rewards/chosen': 10.537179946899414, 'rewards/rejected': -1.1332082748413086, 'rewards/accuracies': 1.0, 'rewards/margins': 11.670389175415039, 'logps/chosen': -223.4911346435547, 'logps/rejected': -94.01990509033203, 'logits/chosen': -2.6800014972686768, 'logits/rejected': -1.4163177013397217, 'epoch': 3.05}\n{'loss': 0.0, 'grad_norm': 0.00149937579408288, 'learning_rate': 2.0660457239627433e-06, 'rewards/chosen': 10.522360801696777, 'rewards/rejected': -1.1286242008209229, 'rewards/accuracies': 1.0, 'rewards/margins': 11.650984764099121, 'logps/chosen': -214.7073974609375, 'logps/rejected': -92.642578125, 'logits/chosen': -2.6962201595306396, 'logits/rejected': -1.3586530685424805, 'epoch': 3.14}\n{'loss': 0.0, 'grad_norm': 0.0010513861197978258, 'learning_rate': 1.960203217612193e-06, 'rewards/chosen': 10.739459991455078, 'rewards/rejected': -1.0914219617843628, 'rewards/accuracies': 1.0, 'rewards/margins': 11.830881118774414, 'logps/chosen': -220.69174194335938, 'logps/rejected': -94.67843627929688, 'logits/chosen': -2.6868767738342285, 'logits/rejected': -1.382515549659729, 'epoch': 3.24}\n{'loss': 0.0, 'grad_norm': 0.000744481454603374, 'learning_rate': 1.854360711261643e-06, 'rewards/chosen': 10.554464340209961, 'rewards/rejected': -1.1087746620178223, 'rewards/accuracies': 1.0, 'rewards/margins': 11.663239479064941, 'logps/chosen': -214.63470458984375, 'logps/rejected': -94.36017608642578, 'logits/chosen': -2.729982376098633, 'logits/rejected': -1.421902060508728, 'epoch': 3.33}\n{'loss': 0.0, 'grad_norm': 0.001265319180674851, 'learning_rate': 1.7485182049110924e-06, 'rewards/chosen': 10.663357734680176, 'rewards/rejected': -1.1239237785339355, 'rewards/accuracies': 1.0, 'rewards/margins': 11.787281036376953, 'logps/chosen': -216.51138305664062, 'logps/rejected': -94.90209197998047, 'logits/chosen': -2.6988589763641357, 'logits/rejected': -1.4041011333465576, 'epoch': 3.43}\n{'loss': 0.0, 'grad_norm': 0.0013770927907899022, 'learning_rate': 1.6426756985605422e-06, 'rewards/chosen': 10.693764686584473, 'rewards/rejected': -1.1206892728805542, 'rewards/accuracies': 1.0, 'rewards/margins': 11.814453125, 'logps/chosen': -214.03892517089844, 'logps/rejected': -93.34362030029297, 'logits/chosen': -2.74151611328125, 'logits/rejected': -1.377084493637085, 'epoch': 3.52}\n{'loss': 0.0, 'grad_norm': 0.0007616530056111515, 'learning_rate': 1.5368331922099917e-06, 'rewards/chosen': 10.662393569946289, 'rewards/rejected': -1.2023091316223145, 'rewards/accuracies': 1.0, 'rewards/margins': 11.864702224731445, 'logps/chosen': -218.5097198486328, 'logps/rejected': -93.18394470214844, 'logits/chosen': -2.7065210342407227, 'logits/rejected': -1.3863520622253418, 'epoch': 3.62}\n{'loss': 0.0, 'grad_norm': 0.0006161553901620209, 'learning_rate': 1.4309906858594413e-06, 'rewards/chosen': 10.678754806518555, 'rewards/rejected': -1.141442060470581, 'rewards/accuracies': 1.0, 'rewards/margins': 11.820196151733398, 'logps/chosen': -215.05662536621094, 'logps/rejected': -94.20645141601562, 'logits/chosen': -2.732304573059082, 'logits/rejected': -1.4161295890808105, 'epoch': 3.71}\n{'loss': 0.0, 'grad_norm': 0.0007063993834890425, 'learning_rate': 1.3251481795088908e-06, 'rewards/chosen': 10.757905006408691, 'rewards/rejected': -1.1788145303726196, 'rewards/accuracies': 1.0, 'rewards/margins': 11.936717987060547, 'logps/chosen': -211.13995361328125, 'logps/rejected': -93.26568603515625, 'logits/chosen': -2.7436439990997314, 'logits/rejected': -1.42643141746521, 'epoch': 3.81}\n{'loss': 0.0, 'grad_norm': 0.0009474524413235486, 'learning_rate': 1.2193056731583406e-06, 'rewards/chosen': 10.683324813842773, 'rewards/rejected': -1.1874001026153564, 'rewards/accuracies': 1.0, 'rewards/margins': 11.87072467803955, 'logps/chosen': -213.24867248535156, 'logps/rejected': -93.45932006835938, 'logits/chosen': -2.741992712020874, 'logits/rejected': -1.3711292743682861, 'epoch': 3.9}\n{'loss': 0.0, 'grad_norm': 0.0007096408517099917, 'learning_rate': 1.1134631668077901e-06, 'rewards/chosen': 10.811097145080566, 'rewards/rejected': -1.1973137855529785, 'rewards/accuracies': 1.0, 'rewards/margins': 12.008411407470703, 'logps/chosen': -213.62709045410156, 'logps/rejected': -92.09535217285156, 'logits/chosen': -2.7622880935668945, 'logits/rejected': -1.4044520854949951, 'epoch': 4.0}\n{'loss': 0.0, 'grad_norm': 0.0017923688283190131, 'learning_rate': 1.0076206604572397e-06, 'rewards/chosen': 10.83184814453125, 'rewards/rejected': -1.1352509260177612, 'rewards/accuracies': 1.0, 'rewards/margins': 11.9670991897583, 'logps/chosen': -217.80340576171875, 'logps/rejected': -95.05892944335938, 'logits/chosen': -2.739464044570923, 'logits/rejected': -1.4180855751037598, 'epoch': 4.1}\n{'loss': 0.0, 'grad_norm': 0.0004233436193317175, 'learning_rate': 9.017781541066894e-07, 'rewards/chosen': 10.961993217468262, 'rewards/rejected': -1.1745336055755615, 'rewards/accuracies': 1.0, 'rewards/margins': 12.136527061462402, 'logps/chosen': -220.74253845214844, 'logps/rejected': -96.08340454101562, 'logits/chosen': -2.732851982116699, 'logits/rejected': -1.4184141159057617, 'epoch': 4.19}\n{'loss': 0.0, 'grad_norm': 0.000407191168051213, 'learning_rate': 7.959356477561389e-07, 'rewards/chosen': 10.882131576538086, 'rewards/rejected': -1.1399613618850708, 'rewards/accuracies': 1.0, 'rewards/margins': 12.02209186553955, 'logps/chosen': -213.1687469482422, 'logps/rejected': -94.31221008300781, 'logits/chosen': -2.749903678894043, 'logits/rejected': -1.4265961647033691, 'epoch': 4.29}\n{'loss': 0.0, 'grad_norm': 0.0004935627803206444, 'learning_rate': 6.900931414055885e-07, 'rewards/chosen': 10.775324821472168, 'rewards/rejected': -1.1515311002731323, 'rewards/accuracies': 1.0, 'rewards/margins': 11.926856994628906, 'logps/chosen': -206.87759399414062, 'logps/rejected': -93.5262680053711, 'logits/chosen': -2.773392915725708, 'logits/rejected': -1.4391136169433594, 'epoch': 4.38}\n{'loss': 0.0, 'grad_norm': 0.00030030938796699047, 'learning_rate': 5.842506350550381e-07, 'rewards/chosen': 10.863163948059082, 'rewards/rejected': -1.2206835746765137, 'rewards/accuracies': 1.0, 'rewards/margins': 12.083847999572754, 'logps/chosen': -214.96295166015625, 'logps/rejected': -92.87797546386719, 'logits/chosen': -2.7552149295806885, 'logits/rejected': -1.4330801963806152, 'epoch': 4.48}\n{'loss': 0.0, 'grad_norm': 0.001689322991296649, 'learning_rate': 4.784081287044878e-07, 'rewards/chosen': 10.764065742492676, 'rewards/rejected': -1.236993432044983, 'rewards/accuracies': 1.0, 'rewards/margins': 12.001059532165527, 'logps/chosen': -211.44818115234375, 'logps/rejected': -94.31653594970703, 'logits/chosen': -2.7873916625976562, 'logits/rejected': -1.4201328754425049, 'epoch': 4.57}\n{'loss': 0.0, 'grad_norm': 0.0006156190647743642, 'learning_rate': 3.7256562235393733e-07, 'rewards/chosen': 10.825516700744629, 'rewards/rejected': -1.188718318939209, 'rewards/accuracies': 1.0, 'rewards/margins': 12.01423454284668, 'logps/chosen': -207.41720581054688, 'logps/rejected': -93.65099334716797, 'logits/chosen': -2.797154426574707, 'logits/rejected': -1.4441105127334595, 'epoch': 4.67}\n{'loss': 0.0, 'grad_norm': 0.0010089253773912787, 'learning_rate': 2.66723116003387e-07, 'rewards/chosen': 10.800676345825195, 'rewards/rejected': -1.2066444158554077, 'rewards/accuracies': 1.0, 'rewards/margins': 12.007320404052734, 'logps/chosen': -216.41073608398438, 'logps/rejected': -92.79219055175781, 'logits/chosen': -2.7286536693573, 'logits/rejected': -1.3840243816375732, 'epoch': 4.76}\n{'loss': 0.0, 'grad_norm': 0.00043792719952762127, 'learning_rate': 1.6088060965283657e-07, 'rewards/chosen': 10.931845664978027, 'rewards/rejected': -1.2178831100463867, 'rewards/accuracies': 1.0, 'rewards/margins': 12.149727821350098, 'logps/chosen': -215.2837371826172, 'logps/rejected': -94.62196350097656, 'logits/chosen': -2.748278856277466, 'logits/rejected': -1.4022117853164673, 'epoch': 4.86}\n{'loss': 0.0, 'grad_norm': 0.0006212029838934541, 'learning_rate': 5.50381033022862e-08, 'rewards/chosen': 10.951327323913574, 'rewards/rejected': -1.2644270658493042, 'rewards/accuracies': 1.0, 'rewards/margins': 12.215751647949219, 'logps/chosen': -215.9133758544922, 'logps/rejected': -94.84992218017578, 'logits/chosen': -2.7705941200256348, 'logits/rejected': -1.360630989074707, 'epoch': 4.95}\n{'train_runtime': 6411.814, 'train_samples_per_second': 3.275, 'train_steps_per_second': 0.409, 'train_loss': 0.013861487213766113, 'epoch': 5.0}\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2625/2625 [1:46:51<00:00,  2.44s/it]\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (0.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (0.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (0.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (0.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (0.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (0.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (0.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚°ø\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (0.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (0.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (1.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (1.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (1.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (1.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (1.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (1.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚°ø\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (1.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (1.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (1.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (1.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (2.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (2.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (2.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (2.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚°ø\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (2.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (2.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (2.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (2.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (2.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (2.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (3.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (3.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚°ø\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (3.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (3.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (3.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (3.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (3.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (3.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (3.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (3.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading history steps 52-52, summary, console lines 62-62 (3.8s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:              train/epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n\u001b[34m\u001b[1mwandb\u001b[0m:        train/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n\u001b[34m\u001b[1mwandb\u001b[0m:          train/grad_norm ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:      train/learning_rate ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:      train/logits/chosen ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:    train/logits/rejected ‚ñà‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ\n\u001b[34m\u001b[1mwandb\u001b[0m:       train/logps/chosen ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá\n\u001b[34m\u001b[1mwandb\u001b[0m:     train/logps/rejected ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ\n\u001b[34m\u001b[1mwandb\u001b[0m:               train/loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: train/rewards/accuracies ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n\u001b[34m\u001b[1mwandb\u001b[0m:     train/rewards/chosen ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n\u001b[34m\u001b[1mwandb\u001b[0m:    train/rewards/margins ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n\u001b[34m\u001b[1mwandb\u001b[0m:   train/rewards/rejected ‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:               total_flos 0\n\u001b[34m\u001b[1mwandb\u001b[0m:              train/epoch 5\n\u001b[34m\u001b[1mwandb\u001b[0m:        train/global_step 2625\n\u001b[34m\u001b[1mwandb\u001b[0m:          train/grad_norm 0.00062\n\u001b[34m\u001b[1mwandb\u001b[0m:      train/learning_rate 0.0\n\u001b[34m\u001b[1mwandb\u001b[0m:      train/logits/chosen -2.77059\n\u001b[34m\u001b[1mwandb\u001b[0m:    train/logits/rejected -1.36063\n\u001b[34m\u001b[1mwandb\u001b[0m:       train/logps/chosen -215.91338\n\u001b[34m\u001b[1mwandb\u001b[0m:     train/logps/rejected -94.84992\n\u001b[34m\u001b[1mwandb\u001b[0m:               train/loss 0\n\u001b[34m\u001b[1mwandb\u001b[0m: train/rewards/accuracies 1\n\u001b[34m\u001b[1mwandb\u001b[0m:     train/rewards/chosen 10.95133\n\u001b[34m\u001b[1mwandb\u001b[0m:    train/rewards/margins 12.21575\n\u001b[34m\u001b[1mwandb\u001b[0m:   train/rewards/rejected -1.26443\n\u001b[34m\u001b[1mwandb\u001b[0m:               train_loss 0.01386\n\u001b[34m\u001b[1mwandb\u001b[0m:            train_runtime 6411.814\n\u001b[34m\u001b[1mwandb\u001b[0m: train_samples_per_second 3.275\n\u001b[34m\u001b[1mwandb\u001b[0m:   train_steps_per_second 0.409\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mqwen3_0.6b_finetuning\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/imnskc/fitness-bot/runs/2wycpige\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/imnskc/fitness-bot\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251005_035612-2wycpige/logs\u001b[0m\n2025-10-05 05:43:14,099 | INFO | üíæ Saving model to outputs...\n2025-10-05 05:43:14,553 | INFO | ‚úÖ Model saved successfully.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!python /kaggle/working/TMLC/week2_unsloth_dpo_qlora/inference.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T05:47:20.394091Z","iopub.execute_input":"2025-10-05T05:47:20.394426Z","iopub.status.idle":"2025-10-05T05:48:24.180087Z","shell.execute_reply.started":"2025-10-05T05:47:20.394384Z","shell.execute_reply":"2025-10-05T05:48:24.179373Z"}},"outputs":[{"name":"stdout","text":"ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n2025-10-05 05:47:27.093971: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759643247.117748     631 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759643247.125177     631 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nü¶• Unsloth Zoo will now patch everything to make training faster!\n2025-10-05 05:47:38,570 | INFO | üîç Loading fine-tuned model from 'outputs'\n2025-10-05 05:47:38,570 | INFO | Loading fine-tuned model from: outputs\n==((====))==  Unsloth 2025.9.11: Fast Qwen3 patching. Transformers: 4.56.2.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.4.0+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.0.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.27.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth 2025.9.11 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n2025-10-05 05:47:44,648 | INFO | ‚úÖ Fine-tuned model loaded for inference\n2025-10-05 05:47:44,648 | INFO | ‚úÖ Fine-tuned model loaded successfully from 'outputs'\n2025-10-05 05:47:44,648 | INFO | üí¨ Running inference for prompt: Common mistakes in micronutrients (iron, calcium, vitamin D) for vegetarian lifters?\n2025-10-05 05:47:44,648 | INFO | üí¨ Starting prediction...\n2025-10-05 05:47:44,648 | INFO | Input prompt: Common mistakes in micronutrients (iron, calcium, vitamin D) for vegetarian lifters?\n2025-10-05 05:47:44,648 | INFO | System prompt: You are a helpful AI fitness assistant for gym-goers and vegetarians.\n2025-10-05 05:47:44,648 | INFO | üîß Applying chat template to messages...\n2025-10-05 05:47:44,649 | INFO | Formatted text: <|im_start|>system\nYou are a helpful AI fitness assistant for gym-goers and vegetarians.<|im_end|>\n<|im_start|>user\nCommon mistakes in micronutrients (iron, calcium, vitamin D) for vegetarian lifters?<|im_end|>\n<|im_start|>assistant\n\n2025-10-05 05:47:44,650 | INFO | Using device: cuda\n2025-10-05 05:47:44,672 | INFO | üîç Checking if adaptive token generation is enabled...\n2025-10-05 05:47:44,673 | INFO | Available VRAM: 15.01 GB\n2025-10-05 05:47:44,673 | INFO | Using max_new_tokens: 1024\n2025-10-05 05:47:44,673 | INFO | Generating text with max_new_tokens=1024...\n2025-10-05 05:48:21,483 | INFO | ‚úÖ Generation complete.\n2025-10-05 05:48:21,484 | INFO | Generated response: <think>\nOkay, the user is asking about common mistakes in micronutrients for vegetarian lifters, specifically focusing on iron, calcium, and vitamin D. Let me start by recalling the key points I learned before. \n\nFirst, I know that vegetarian diets are based on whole foods, so the focus should be on whole foods, not processed ones. The user might be making the mistake of relying on supplements instead of whole foods. For example, they might think iron comes from plant foods but actually get it from other sources. \n\nNext, calcium and vitamin D are crucial for bone health. I remember that vegetarians often don't get enough of these, so I need to explain how they can get them through fortified foods. Maybe mention specific examples like fortified dairy or certain foods. \n\nI should also address the importance of a balanced diet, avoiding processed foods, and consulting a professional if they're unsure. Let me structure this step by step: start with iron, then calcium and vitamin D, each with their sources and potential issues. Make sure the language is clear and helpful, maybe use bullet points for lists. Finally, check if they need more details on how to increase specific nutrients, like calculating daily needs or checking labels.\n</think>\n\nHere are common mistakes to avoid when maintaining a balanced diet and training for weightlifting, especially for those following a vegetarian or vegan diet:\n\n### **1. Iron**\n- **Mistake**: Believes iron only comes from animal products, but plant-based sources (like leafy greens) are often low in bioavailability.  \n- **Solution**: Consume foods like fortified plant milks, iron-folic combined foods (e.g., Heinz IronForte), or take a supplement with vitamin C (like citrus fruits) for maximum absorption.  \n\n### **2. Calcium**\n- **Mistake**: Over-reliance on dairy or supplements without whole foods (like fortified plant milks, fortified eggs, or tofu).  \n- **Solution**: Include dairy (if you eat it), fortified plant milks, or supplements with calcium and vitamin D, as advised by a registered dietitian.  \n\n### **3. Vitamin D**\n- **Mistake**: Underestimates the importance of sunlight and fortified foods (e.g., fatty fish, eggs) for absorption.  \n- **Solution**: Include fatty fish (like salmon), eggs, or fortified dairy, and consider sun exposure (if not vegan) for natural vitamin D.  \n\n### **General Tips**:\n- **Whole foods** (fruits, vegetables, legumes) are key for fiber, protein, and other nutrients.  \n- **Hydration** and a diverse protein source (fish, beans, nuts) are vital.  \n- **Consult a professional** for personalized guidance, especially if unsure about daily nutrient needs or supplementation.  \n\nBy addressing these basics, vegetarians can avoid common pitfalls and build a strong, balanced diet for weightlifting.\n2025-10-05 05:48:21,484 | INFO | ‚úÖ Prediction completed\n\nüí° Prediction:\n <think>\nOkay, the user is asking about common mistakes in micronutrients for vegetarian lifters, specifically focusing on iron, calcium, and vitamin D. Let me start by recalling the key points I learned before. \n\nFirst, I know that vegetarian diets are based on whole foods, so the focus should be on whole foods, not processed ones. The user might be making the mistake of relying on supplements instead of whole foods. For example, they might think iron comes from plant foods but actually get it from other sources. \n\nNext, calcium and vitamin D are crucial for bone health. I remember that vegetarians often don't get enough of these, so I need to explain how they can get them through fortified foods. Maybe mention specific examples like fortified dairy or certain foods. \n\nI should also address the importance of a balanced diet, avoiding processed foods, and consulting a professional if they're unsure. Let me structure this step by step: start with iron, then calcium and vitamin D, each with their sources and potential issues. Make sure the language is clear and helpful, maybe use bullet points for lists. Finally, check if they need more details on how to increase specific nutrients, like calculating daily needs or checking labels.\n</think>\n\nHere are common mistakes to avoid when maintaining a balanced diet and training for weightlifting, especially for those following a vegetarian or vegan diet:\n\n### **1. Iron**\n- **Mistake**: Believes iron only comes from animal products, but plant-based sources (like leafy greens) are often low in bioavailability.  \n- **Solution**: Consume foods like fortified plant milks, iron-folic combined foods (e.g., Heinz IronForte), or take a supplement with vitamin C (like citrus fruits) for maximum absorption.  \n\n### **2. Calcium**\n- **Mistake**: Over-reliance on dairy or supplements without whole foods (like fortified plant milks, fortified eggs, or tofu).  \n- **Solution**: Include dairy (if you eat it), fortified plant milks, or supplements with calcium and vitamin D, as advised by a registered dietitian.  \n\n### **3. Vitamin D**\n- **Mistake**: Underestimates the importance of sunlight and fortified foods (e.g., fatty fish, eggs) for absorption.  \n- **Solution**: Include fatty fish (like salmon), eggs, or fortified dairy, and consider sun exposure (if not vegan) for natural vitamin D.  \n\n### **General Tips**:\n- **Whole foods** (fruits, vegetables, legumes) are key for fiber, protein, and other nutrients.  \n- **Hydration** and a diverse protein source (fish, beans, nuts) are vital.  \n- **Consult a professional** for personalized guidance, especially if unsure about daily nutrient needs or supplementation.  \n\nBy addressing these basics, vegetarians can avoid common pitfalls and build a strong, balanced diet for weightlifting.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import os\nos.chdir(\"/kaggle/working/TMLC/week2_unsloth_dpo_qlora/\")\nfrom configurations.config import INFERENCE_MODEL_PATH, INFERENCE_MAX_SEQ_LENGTH, INFERENCE_LOAD_IN_4BIT, logger\nfrom core.model_loader import load_finetuned_model\nfrom core.inference_utils import predict\n\nif __name__ == \"__main__\":\n    logger.info(f\"üîç Loading fine-tuned model from '{INFERENCE_MODEL_PATH}'\")\n    try:\n        INFERENCE_MODEL_PATH = \"/kaggle/working/outputs/\"\n        model, tokenizer = load_finetuned_model(\n            INFERENCE_MODEL_PATH, INFERENCE_MAX_SEQ_LENGTH, INFERENCE_LOAD_IN_4BIT\n        )\n        logger.info(f\"‚úÖ Fine-tuned model loaded successfully from '{INFERENCE_MODEL_PATH}'\")\n    except Exception as e:\n        logger.error(f\"‚ùå Failed to load model: {e}\")\n        raise\n\n    input_prompt = \"What are the most effective leg exercies. Just give me names of 3 exercises with no explanation.\"\n    logger.info(f\"üí¨ Running inference for prompt: {input_prompt}\")\n\n    try:\n        response = predict(model, tokenizer, input_prompt)\n        logger.info(f\"‚úÖ Prediction completed\")\n        print(\"\\nüí° Prediction:\\n\", response)\n    except Exception as e:\n        logger.error(f\"‚ùå Inference failed: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T05:54:19.355867Z","iopub.execute_input":"2025-10-05T05:54:19.356554Z","iopub.status.idle":"2025-10-05T05:54:49.908061Z","shell.execute_reply.started":"2025-10-05T05:54:19.356531Z","shell.execute_reply":"2025-10-05T05:54:49.907351Z"}},"outputs":[{"name":"stderr","text":"2025-10-05 05:54:19,359 | INFO | üîç Loading fine-tuned model from 'outputs'\n2025-10-05 05:54:19,360 | INFO | Loading fine-tuned model from: /kaggle/working/outputs/\n","output_type":"stream"},{"name":"stdout","text":"==((====))==  Unsloth 2025.9.11: Fast Qwen3 patching. Transformers: 4.56.2.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.4.0+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.0.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.27.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"name":"stderr","text":"Unsloth 2025.9.11 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n2025-10-05 05:54:25,717 | INFO | ‚úÖ Fine-tuned model loaded for inference\n2025-10-05 05:54:25,717 | INFO | ‚úÖ Fine-tuned model loaded successfully from '/kaggle/working/outputs/'\n2025-10-05 05:54:25,718 | INFO | üí¨ Running inference for prompt: What are the most effective leg exercies. Just give me names of 3 exercises with no explanation.\n2025-10-05 05:54:25,718 | INFO | üí¨ Starting prediction...\n2025-10-05 05:54:25,719 | INFO | Input prompt: What are the most effective leg exercies. Just give me names of 3 exercises with no explanation.\n2025-10-05 05:54:25,720 | INFO | System prompt: You are a helpful AI fitness assistant for gym-goers and vegetarians.\n2025-10-05 05:54:25,720 | INFO | üîß Applying chat template to messages...\n2025-10-05 05:54:25,721 | INFO | Formatted text: <|im_start|>system\nYou are a helpful AI fitness assistant for gym-goers and vegetarians.<|im_end|>\n<|im_start|>user\nWhat are the most effective leg exercies. Just give me names of 3 exercises with no explanation.<|im_end|>\n<|im_start|>assistant\n\n2025-10-05 05:54:25,723 | INFO | Using device: cuda\n2025-10-05 05:54:25,746 | INFO | üîç Checking if adaptive token generation is enabled...\n2025-10-05 05:54:25,747 | INFO | Available VRAM: 15.01 GB\n2025-10-05 05:54:25,747 | INFO | Using max_new_tokens: 1024\n2025-10-05 05:54:25,748 | INFO | Generating text with max_new_tokens=1024...\n2025-10-05 05:54:49,902 | INFO | ‚úÖ Generation complete.\n2025-10-05 05:54:49,903 | INFO | Generated response: <think>\nOkay, the user is asking for the most effective leg exercises. They want three exercises without any explanation. Let me think. First, I need to recall common exercises for the legs. The user is likely a gym-goer, so they might be looking for something quick and efficient. \n\nI should consider compound exercises that target multiple muscle groups. For legs, squats, push-ups, and planks come to mind. But the user wants three with no explanation. So, I can list them but mention that each has a brief explanation. Wait, the user said \"no explanation\" so I can't add that. \n\nLet me check if I'm mixing up different muscles. Squats are good for strength and stability, push-ups for upper body, and planks for core. But the user wants three leg exercises. So, maybe: 1. Squat, 2. Deadlift, 3. Leg Press. But need to confirm if these are commonly used. \n\nAlternatively, maybe planks, push-ups, and squats. But the user said no explanation. So I'll go with three exercises, each with a brief note. Wait, the original answer had three exercises but without explanation. The user wants that. So I need to make sure each is a quick, effective exercise. \n\nLet me confirm the most effective leg exercises. Squats, Deadlift, and Leg Press. Each is a compound exercise, so they're effective for strength and stability. I can present them here.\n</think>\n\n1. **Squats** ‚Äì Targets the quads, glutes, and core, building strength and power.  \n2. **Deadlifts** ‚Äì Works the back, shoulders, and core, enhancing stability and form.  \n3. **Leg Press** ‚Äì Targets the lower back, hips, and shoulders, improving posture and balance.\n2025-10-05 05:54:49,904 | INFO | ‚úÖ Prediction completed\n","output_type":"stream"},{"name":"stdout","text":"\nüí° Prediction:\n <think>\nOkay, the user is asking for the most effective leg exercises. They want three exercises without any explanation. Let me think. First, I need to recall common exercises for the legs. The user is likely a gym-goer, so they might be looking for something quick and efficient. \n\nI should consider compound exercises that target multiple muscle groups. For legs, squats, push-ups, and planks come to mind. But the user wants three with no explanation. So, I can list them but mention that each has a brief explanation. Wait, the user said \"no explanation\" so I can't add that. \n\nLet me check if I'm mixing up different muscles. Squats are good for strength and stability, push-ups for upper body, and planks for core. But the user wants three leg exercises. So, maybe: 1. Squat, 2. Deadlift, 3. Leg Press. But need to confirm if these are commonly used. \n\nAlternatively, maybe planks, push-ups, and squats. But the user said no explanation. So I'll go with three exercises, each with a brief note. Wait, the original answer had three exercises but without explanation. The user wants that. So I need to make sure each is a quick, effective exercise. \n\nLet me confirm the most effective leg exercises. Squats, Deadlift, and Leg Press. Each is a compound exercise, so they're effective for strength and stability. I can present them here.\n</think>\n\n1. **Squats** ‚Äì Targets the quads, glutes, and core, building strength and power.  \n2. **Deadlifts** ‚Äì Works the back, shoulders, and core, enhancing stability and form.  \n3. **Leg Press** ‚Äì Targets the lower back, hips, and shoulders, improving posture and balance.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import os\nos.chdir(\"/kaggle/working/TMLC/week2_unsloth_dpo_qlora/\")\nfrom configurations.config import INFERENCE_MODEL_PATH, INFERENCE_MAX_SEQ_LENGTH, INFERENCE_LOAD_IN_4BIT, logger\nfrom core.model_loader import load_finetuned_model\nfrom core.inference_utils import predict\n\nif __name__ == \"__main__\":\n    logger.info(f\"üîç Loading fine-tuned model from '{INFERENCE_MODEL_PATH}'\")\n    try:\n        INFERENCE_MODEL_PATH = \"/kaggle/working/outputs/\"\n        model, tokenizer = load_finetuned_model(\n            INFERENCE_MODEL_PATH, INFERENCE_MAX_SEQ_LENGTH, INFERENCE_LOAD_IN_4BIT\n        )\n        logger.info(f\"‚úÖ Fine-tuned model loaded successfully from '{INFERENCE_MODEL_PATH}'\")\n    except Exception as e:\n        logger.error(f\"‚ùå Failed to load model: {e}\")\n        raise\n\n    input_prompt = \"Is whey protein good enough to consume. Just say yes or no?\"\n    logger.info(f\"üí¨ Running inference for prompt: {input_prompt}\")\n\n    try:\n        response = predict(model, tokenizer, input_prompt)\n        logger.info(f\"‚úÖ Prediction completed\")\n        print(\"\\nüí° Prediction:\\n\", response)\n    except Exception as e:\n        logger.error(f\"‚ùå Inference failed: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T05:56:23.143791Z","iopub.execute_input":"2025-10-05T05:56:23.144507Z","iopub.status.idle":"2025-10-05T05:56:52.815346Z","shell.execute_reply.started":"2025-10-05T05:56:23.144482Z","shell.execute_reply":"2025-10-05T05:56:52.814763Z"}},"outputs":[{"name":"stderr","text":"2025-10-05 05:56:23,147 | INFO | üîç Loading fine-tuned model from 'outputs'\n2025-10-05 05:56:23,149 | INFO | Loading fine-tuned model from: /kaggle/working/outputs/\n","output_type":"stream"},{"name":"stdout","text":"==((====))==  Unsloth 2025.9.11: Fast Qwen3 patching. Transformers: 4.56.2.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.4.0+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.0.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.27.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"name":"stderr","text":"2025-10-05 05:56:29,190 | INFO | ‚úÖ Fine-tuned model loaded for inference\n2025-10-05 05:56:29,191 | INFO | ‚úÖ Fine-tuned model loaded successfully from '/kaggle/working/outputs/'\n2025-10-05 05:56:29,191 | INFO | üí¨ Running inference for prompt: Is whey protein good enough to consume. Just say yes or no?\n2025-10-05 05:56:29,192 | INFO | üí¨ Starting prediction...\n2025-10-05 05:56:29,193 | INFO | Input prompt: Is whey protein good enough to consume. Just say yes or no?\n2025-10-05 05:56:29,193 | INFO | System prompt: You are a helpful AI fitness assistant for gym-goers and vegetarians.\n2025-10-05 05:56:29,194 | INFO | üîß Applying chat template to messages...\n2025-10-05 05:56:29,195 | INFO | Formatted text: <|im_start|>system\nYou are a helpful AI fitness assistant for gym-goers and vegetarians.<|im_end|>\n<|im_start|>user\nIs whey protein good enough to consume. Just say yes or no?<|im_end|>\n<|im_start|>assistant\n\n2025-10-05 05:56:29,197 | INFO | Using device: cuda\n2025-10-05 05:56:29,219 | INFO | üîç Checking if adaptive token generation is enabled...\n2025-10-05 05:56:29,220 | INFO | Available VRAM: 14.27 GB\n2025-10-05 05:56:29,220 | INFO | Using max_new_tokens: 1024\n2025-10-05 05:56:29,222 | INFO | Generating text with max_new_tokens=1024...\n2025-10-05 05:56:52,809 | INFO | ‚úÖ Generation complete.\n2025-10-05 05:56:52,811 | INFO | Generated response: <think>\nOkay, the user is asking if whey protein is good enough to consume. They want a simple \"yes\" or \"no\" answer. Let me break this down.\n\nFirst, I should consider the common perception. Many people believe that whey is the best, but I need to be honest. There are other factors to consider. Let me check the information I have. \n\nWhey protein is a good source of high-quality protein, containing all the amino acids in the right ratios. It's also rich in essential fats and is easy to digest. However, there are other benefits like muscle synthesis and a lean body build. \n\nBut the user might be looking for a quick answer without going into details. So I should state \"yes\" but mention that it's not the only option. Wait, the user just wants \"yes\" or \"no\". Oh, right, the answer should be \"yes\" and then explain. But the original question is \"Is whey protein good enough to consume. Just say yes or no?\" So the answer is \"yes\". But then I need to add a bit more context. Wait, the user might be expecting a \"yes\" answer but wants more. Let me make sure I'm not missing anything. \n\nI think the correct approach is to state \"yes\" and then explain why, but since the user only asked \"Is whey protein good enough... just say yes or no?\" the answer is \"yes\". But maybe the user wants a concise \"yes\" and a brief explanation. Let me confirm the response.\n</think>\n\nYes. Whey protein is generally good enough to consume for many people, as it is a high-quality, complete protein source rich in amino acids, essential fats, and important nutrients. It supports muscle growth and recovery, and can be an effective supplement when used in combination with a balanced diet. However, individual needs may vary, so consulting a healthcare provider is always a good idea.\n2025-10-05 05:56:52,812 | INFO | ‚úÖ Prediction completed\n","output_type":"stream"},{"name":"stdout","text":"\nüí° Prediction:\n <think>\nOkay, the user is asking if whey protein is good enough to consume. They want a simple \"yes\" or \"no\" answer. Let me break this down.\n\nFirst, I should consider the common perception. Many people believe that whey is the best, but I need to be honest. There are other factors to consider. Let me check the information I have. \n\nWhey protein is a good source of high-quality protein, containing all the amino acids in the right ratios. It's also rich in essential fats and is easy to digest. However, there are other benefits like muscle synthesis and a lean body build. \n\nBut the user might be looking for a quick answer without going into details. So I should state \"yes\" but mention that it's not the only option. Wait, the user just wants \"yes\" or \"no\". Oh, right, the answer should be \"yes\" and then explain. But the original question is \"Is whey protein good enough to consume. Just say yes or no?\" So the answer is \"yes\". But then I need to add a bit more context. Wait, the user might be expecting a \"yes\" answer but wants more. Let me make sure I'm not missing anything. \n\nI think the correct approach is to state \"yes\" and then explain why, but since the user only asked \"Is whey protein good enough... just say yes or no?\" the answer is \"yes\". But maybe the user wants a concise \"yes\" and a brief explanation. Let me confirm the response.\n</think>\n\nYes. Whey protein is generally good enough to consume for many people, as it is a high-quality, complete protein source rich in amino acids, essential fats, and important nutrients. It supports muscle growth and recovery, and can be an effective supplement when used in combination with a balanced diet. However, individual needs may vary, so consulting a healthcare provider is always a good idea.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!zip -r /kaggle/working/unsloth_dpo_qlora.zip /kaggle/working/huggingface_tokenizers_cache /kaggle/working/logs /kaggle/working/outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T06:55:45.581582Z","iopub.execute_input":"2025-10-05T06:55:45.581846Z","iopub.status.idle":"2025-10-05T06:56:12.466144Z","shell.execute_reply.started":"2025-10-05T06:55:45.581826Z","shell.execute_reply":"2025-10-05T06:56:12.465444Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/huggingface_tokenizers_cache/ (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/.locks/ (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/.locks/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/ (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/.locks/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/370b963bb5f261aa995ffeeb88d83a1d71dbb084.lock (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/.locks/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/4783fe10ac3adce15ac8f358ef5462739852c569.lock (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/.locks/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/b54f9135e44c1e81047e8d05cb027af8bc039eed.lock (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/.locks/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/9b8043f10c758210957b050c77f14d6282f33a52.lock (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/.locks/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/aeb13307a71acd8fe81861d94ad54ab689df773318809eed3cbe794b4492dae4.lock (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/.locks/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/3bb1381ea04aa2c8cbae85aef2794a3a23062292.lock (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/.locks/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/31349551d90c7606f325fe0f11bbb8bd5fa0d7c7.lock (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/ (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/refs/ (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/refs/main (deflated 3%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/snapshots/ (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/snapshots/e86f5289a2ca40ab7114071959dc2079cf97cb3c/ (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/snapshots/e86f5289a2ca40ab7114071959dc2079cf97cb3c/vocab.json","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":" (deflated 61%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/snapshots/e86f5289a2ca40ab7114071959dc2079cf97cb3c/chat_template.jinja (deflated 77%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/snapshots/e86f5289a2ca40ab7114071959dc2079cf97cb3c/merges.txt (deflated 57%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/snapshots/e86f5289a2ca40ab7114071959dc2079cf97cb3c/added_tokens.json (deflated 68%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/snapshots/e86f5289a2ca40ab7114071959dc2079cf97cb3c/tokenizer.json (deflated 81%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/snapshots/e86f5289a2ca40ab7114071959dc2079cf97cb3c/tokenizer_config.json (deflated 84%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/snapshots/e86f5289a2ca40ab7114071959dc2079cf97cb3c/special_tokens_map.json (deflated 69%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/blobs/ (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/blobs/9b8043f10c758210957b050c77f14d6282f33a52 (deflated 69%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/blobs/aeb13307a71acd8fe81861d94ad54ab689df773318809eed3cbe794b4492dae4 (deflated 81%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/blobs/3bb1381ea04aa2c8cbae85aef2794a3a23062292 (deflated 84%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/blobs/370b963bb5f261aa995ffeeb88d83a1d71dbb084 (deflated 77%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/blobs/31349551d90c7606f325fe0f11bbb8bd5fa0d7c7 (deflated 57%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/blobs/b54f9135e44c1e81047e8d05cb027af8bc039eed (deflated 68%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--Qwen3-0.6B-unsloth-bnb-4bit/blobs/4783fe10ac3adce15ac8f358ef5462739852c569 (deflated 61%)\n  adding: kaggle/working/logs/ (stored 0%)\n  adding: kaggle/working/logs/fitness_bot.log (deflated 63%)\n  adding: kaggle/working/outputs/ (stored 0%)\n  adding: kaggle/working/outputs/checkpoint-500/ (stored 0%)\n  adding: kaggle/working/outputs/checkpoint-500/training_args.bin (deflated 51%)\n  adding: kaggle/working/outputs/checkpoint-500/vocab.json (deflated 61%)\n  adding: kaggle/working/outputs/checkpoint-500/README.md (deflated 66%)\n  adding: kaggle/working/outputs/checkpoint-500/optimizer.pt (deflated 8%)\n  adding: kaggle/working/outputs/checkpoint-500/chat_template.jinja (deflated 77%)\n  adding: kaggle/working/outputs/checkpoint-500/merges.txt (deflated 57%)\n  adding: kaggle/working/outputs/checkpoint-500/scheduler.pt (deflated 56%)\n  adding: kaggle/working/outputs/checkpoint-500/trainer_state.json (deflated 71%)\n  adding: kaggle/working/outputs/checkpoint-500/added_tokens.json (deflated 68%)\n  adding: kaggle/working/outputs/checkpoint-500/scaler.pt (deflated 60%)\n  adding: kaggle/working/outputs/checkpoint-500/tokenizer.json (deflated 81%)\n  adding: kaggle/working/outputs/checkpoint-500/tokenizer_config.json (deflated 90%)\n  adding: kaggle/working/outputs/checkpoint-500/rng_state.pth (deflated 25%)\n  adding: kaggle/working/outputs/checkpoint-500/adapter_config.json (deflated 56%)\n  adding: kaggle/working/outputs/checkpoint-500/special_tokens_map.json (deflated 69%)\n  adding: kaggle/working/outputs/checkpoint-500/adapter_model.safetensors (deflated 8%)\n  adding: kaggle/working/outputs/vocab.json (deflated 61%)\n  adding: kaggle/working/outputs/README.md (deflated 47%)\n  adding: kaggle/working/outputs/checkpoint-2625/ (stored 0%)\n  adding: kaggle/working/outputs/checkpoint-2625/training_args.bin (deflated 51%)\n  adding: kaggle/working/outputs/checkpoint-2625/vocab.json (deflated 61%)\n  adding: kaggle/working/outputs/checkpoint-2625/README.md (deflated 66%)\n  adding: kaggle/working/outputs/checkpoint-2625/optimizer.pt (deflated 8%)\n  adding: kaggle/working/outputs/checkpoint-2625/chat_template.jinja (deflated 77%)\n  adding: kaggle/working/outputs/checkpoint-2625/merges.txt (deflated 57%)\n  adding: kaggle/working/outputs/checkpoint-2625/scheduler.pt (deflated 56%)\n  adding: kaggle/working/outputs/checkpoint-2625/trainer_state.json (deflated 77%)\n  adding: kaggle/working/outputs/checkpoint-2625/added_tokens.json (deflated 68%)\n  adding: kaggle/working/outputs/checkpoint-2625/scaler.pt (deflated 60%)\n  adding: kaggle/working/outputs/checkpoint-2625/tokenizer.json (deflated 81%)\n  adding: kaggle/working/outputs/checkpoint-2625/tokenizer_config.json (deflated 90%)\n  adding: kaggle/working/outputs/checkpoint-2625/rng_state.pth (deflated 25%)\n  adding: kaggle/working/outputs/checkpoint-2625/adapter_config.json (deflated 56%)\n  adding: kaggle/working/outputs/checkpoint-2625/special_tokens_map.json (deflated 69%)\n  adding: kaggle/working/outputs/checkpoint-2625/adapter_model.safetensors (deflated 8%)\n  adding: kaggle/working/outputs/chat_template.jinja (deflated 77%)\n  adding: kaggle/working/outputs/merges.txt (deflated 57%)\n  adding: kaggle/working/outputs/added_tokens.json (deflated 68%)\n  adding: kaggle/working/outputs/checkpoint-2000/ (stored 0%)\n  adding: kaggle/working/outputs/checkpoint-2000/training_args.bin (deflated 51%)\n  adding: kaggle/working/outputs/checkpoint-2000/vocab.json (deflated 61%)\n  adding: kaggle/working/outputs/checkpoint-2000/README.md (deflated 66%)\n  adding: kaggle/working/outputs/checkpoint-2000/optimizer.pt (deflated 8%)\n  adding: kaggle/working/outputs/checkpoint-2000/chat_template.jinja (deflated 77%)\n  adding: kaggle/working/outputs/checkpoint-2000/merges.txt (deflated 57%)\n  adding: kaggle/working/outputs/checkpoint-2000/scheduler.pt (deflated 55%)\n  adding: kaggle/working/outputs/checkpoint-2000/trainer_state.json (deflated 77%)\n  adding: kaggle/working/outputs/checkpoint-2000/added_tokens.json (deflated 68%)\n  adding: kaggle/working/outputs/checkpoint-2000/scaler.pt (deflated 60%)\n  adding: kaggle/working/outputs/checkpoint-2000/tokenizer.json (deflated 81%)\n  adding: kaggle/working/outputs/checkpoint-2000/tokenizer_config.json (deflated 90%)\n  adding: kaggle/working/outputs/checkpoint-2000/rng_state.pth (deflated 25%)\n  adding: kaggle/working/outputs/checkpoint-2000/adapter_config.json (deflated 56%)\n  adding: kaggle/working/outputs/checkpoint-2000/special_tokens_map.json (deflated 69%)\n  adding: kaggle/working/outputs/checkpoint-2000/adapter_model.safetensors (deflated 8%)\n  adding: kaggle/working/outputs/tokenizer.json (deflated 81%)\n  adding: kaggle/working/outputs/checkpoint-1500/ (stored 0%)\n  adding: kaggle/working/outputs/checkpoint-1500/training_args.bin (deflated 51%)\n  adding: kaggle/working/outputs/checkpoint-1500/vocab.json (deflated 61%)\n  adding: kaggle/working/outputs/checkpoint-1500/README.md (deflated 66%)\n  adding: kaggle/working/outputs/checkpoint-1500/optimizer.pt (deflated 8%)\n  adding: kaggle/working/outputs/checkpoint-1500/chat_template.jinja (deflated 77%)\n  adding: kaggle/working/outputs/checkpoint-1500/merges.txt (deflated 57%)\n  adding: kaggle/working/outputs/checkpoint-1500/scheduler.pt (deflated 55%)\n  adding: kaggle/working/outputs/checkpoint-1500/trainer_state.json (deflated 76%)\n  adding: kaggle/working/outputs/checkpoint-1500/added_tokens.json (deflated 68%)\n  adding: kaggle/working/outputs/checkpoint-1500/scaler.pt (deflated 60%)\n  adding: kaggle/working/outputs/checkpoint-1500/tokenizer.json (deflated 81%)\n  adding: kaggle/working/outputs/checkpoint-1500/tokenizer_config.json (deflated 90%)\n  adding: kaggle/working/outputs/checkpoint-1500/rng_state.pth (deflated 25%)\n  adding: kaggle/working/outputs/checkpoint-1500/adapter_config.json (deflated 56%)\n  adding: kaggle/working/outputs/checkpoint-1500/special_tokens_map.json (deflated 69%)\n  adding: kaggle/working/outputs/checkpoint-1500/adapter_model.safetensors (deflated 8%)\n  adding: kaggle/working/outputs/tokenizer_config.json (deflated 90%)\n  adding: kaggle/working/outputs/checkpoint-1000/ (stored 0%)\n  adding: kaggle/working/outputs/checkpoint-1000/training_args.bin (deflated 51%)\n  adding: kaggle/working/outputs/checkpoint-1000/vocab.json (deflated 61%)\n  adding: kaggle/working/outputs/checkpoint-1000/README.md (deflated 66%)\n  adding: kaggle/working/outputs/checkpoint-1000/optimizer.pt (deflated 8%)\n  adding: kaggle/working/outputs/checkpoint-1000/chat_template.jinja (deflated 77%)\n  adding: kaggle/working/outputs/checkpoint-1000/merges.txt (deflated 57%)\n  adding: kaggle/working/outputs/checkpoint-1000/scheduler.pt (deflated 56%)\n  adding: kaggle/working/outputs/checkpoint-1000/trainer_state.json (deflated 75%)\n  adding: kaggle/working/outputs/checkpoint-1000/added_tokens.json (deflated 68%)\n  adding: kaggle/working/outputs/checkpoint-1000/scaler.pt (deflated 60%)\n  adding: kaggle/working/outputs/checkpoint-1000/tokenizer.json (deflated 81%)\n  adding: kaggle/working/outputs/checkpoint-1000/tokenizer_config.json (deflated 90%)\n  adding: kaggle/working/outputs/checkpoint-1000/rng_state.pth (deflated 25%)\n  adding: kaggle/working/outputs/checkpoint-1000/adapter_config.json (deflated 56%)\n  adding: kaggle/working/outputs/checkpoint-1000/special_tokens_map.json (deflated 69%)\n  adding: kaggle/working/outputs/checkpoint-1000/adapter_model.safetensors (deflated 8%)\n  adding: kaggle/working/outputs/adapter_config.json (deflated 56%)\n  adding: kaggle/working/outputs/special_tokens_map.json (deflated 69%)\n  adding: kaggle/working/outputs/checkpoint-2500/ (stored 0%)\n  adding: kaggle/working/outputs/checkpoint-2500/training_args.bin (deflated 51%)\n  adding: kaggle/working/outputs/checkpoint-2500/vocab.json (deflated 61%)\n  adding: kaggle/working/outputs/checkpoint-2500/README.md (deflated 66%)\n  adding: kaggle/working/outputs/checkpoint-2500/optimizer.pt (deflated 8%)\n  adding: kaggle/working/outputs/checkpoint-2500/chat_template.jinja (deflated 77%)\n  adding: kaggle/working/outputs/checkpoint-2500/merges.txt (deflated 57%)\n  adding: kaggle/working/outputs/checkpoint-2500/scheduler.pt (deflated 55%)\n  adding: kaggle/working/outputs/checkpoint-2500/trainer_state.json (deflated 77%)\n  adding: kaggle/working/outputs/checkpoint-2500/added_tokens.json (deflated 68%)\n  adding: kaggle/working/outputs/checkpoint-2500/scaler.pt (deflated 60%)\n  adding: kaggle/working/outputs/checkpoint-2500/tokenizer.json (deflated 81%)\n  adding: kaggle/working/outputs/checkpoint-2500/tokenizer_config.json (deflated 90%)\n  adding: kaggle/working/outputs/checkpoint-2500/rng_state.pth (deflated 25%)\n  adding: kaggle/working/outputs/checkpoint-2500/adapter_config.json (deflated 56%)\n  adding: kaggle/working/outputs/checkpoint-2500/special_tokens_map.json (deflated 69%)\n  adding: kaggle/working/outputs/checkpoint-2500/adapter_model.safetensors (deflated 8%)\n  adding: kaggle/working/outputs/adapter_model.safetensors (deflated 8%)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import os\nos.listdir(\"/kaggle/working/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T06:56:52.578907Z","iopub.execute_input":"2025-10-05T06:56:52.579563Z","iopub.status.idle":"2025-10-05T06:56:52.585051Z","shell.execute_reply.started":"2025-10-05T06:56:52.579535Z","shell.execute_reply":"2025-10-05T06:56:52.584290Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"['v0.1_unsloth_dpo_qlora.zip',\n 'unsloth_dpo_qlora.zip',\n 'wandb',\n 'TMLC',\n '.virtual_documents',\n 'outputs',\n 'huggingface_tokenizers_cache',\n 'logs',\n 'unsloth_compiled_cache']"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Create a clickable download link\nFileLink(\"/kaggle/working/unsloth_dpo_qlora.zip\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T06:57:03.465846Z","iopub.execute_input":"2025-10-05T06:57:03.466611Z","iopub.status.idle":"2025-10-05T06:57:03.471380Z","shell.execute_reply.started":"2025-10-05T06:57:03.466586Z","shell.execute_reply":"2025-10-05T06:57:03.470597Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/unsloth_dpo_qlora.zip","text/html":"<a href='/kaggle/working/unsloth_dpo_qlora.zip' target='_blank'>/kaggle/working/unsloth_dpo_qlora.zip</a><br>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"from IPython.display import HTML\n\nHTML('<a href=\"/kaggle/working/unsloth_dpo_qlora.zip\" download>Download v0.1_unsloth_dpo_qlora.zip</a>')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T06:58:09.307961Z","iopub.execute_input":"2025-10-05T06:58:09.308256Z","iopub.status.idle":"2025-10-05T06:58:09.313109Z","shell.execute_reply.started":"2025-10-05T06:58:09.308234Z","shell.execute_reply":"2025-10-05T06:58:09.312454Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<a href=\"/kaggle/working/unsloth_dpo_qlora.zip\" download>Download v0.1_unsloth_dpo_qlora.zip</a>"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"import os\n\nos.remove(\"/kaggle/working/v0.1_unsloth_dpo_qlora.zip\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T07:00:24.133937Z","iopub.execute_input":"2025-10-05T07:00:24.134665Z","iopub.status.idle":"2025-10-05T07:00:24.217062Z","shell.execute_reply.started":"2025-10-05T07:00:24.134639Z","shell.execute_reply":"2025-10-05T07:00:24.216360Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}